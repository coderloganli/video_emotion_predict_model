{"cells":[{"cell_type":"code","source":["import os\n","import cv2\n","import json\n","import re\n","import gc\n","import torch\n","import numpy as np\n","from pathlib import Path\n","from PIL import Image\n","from tqdm import tqdm\n","from transformers import (\n","    AutoProcessor,\n","    AutoModelForVision2Seq,\n","    BitsAndBytesConfig\n",")\n","from sentence_transformers import SentenceTransformer\n","\n","def check_device():\n","    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","DEVICE = check_device()"],"metadata":{"id":"Oz0zDUMOyzfL"},"id":"Oz0zDUMOyzfL","execution_count":null,"outputs":[]},{"cell_type":"code","source":["CONFIG = {\n","    \"VIDEO_ROOT\": Path(\"data/LIRIS-ACCEDE-data/data\"),\n","    \"FRAME_OUTPUT_ROOT\": Path(\"data/keyframes\"),\n","    \"DESC_OUTPUT_ROOT\": Path(\"data/semantic/frame_description\"),\n","    \"FEATURE_OUTPUT_ROOT\": Path(\"data/features_semantic\"),\n","    \"NUM_FRAMES\": 4,\n","    \"LLAVA_MODEL_ID\": \"llava-hf/llava-v1.6-mistral-7b-hf\",\n","    \"EMBEDDING_MODEL_ID\": \"intfloat/e5-large\",\n","    \"MAX_NEW_TOKENS\": 256,\n","    \"BATCH_SIZE\": 50\n","}\n","\n","for path_key in [\"FRAME_OUTPUT_ROOT\", \"DESC_OUTPUT_ROOT\", \"FEATURE_OUTPUT_ROOT\"]:\n","    CONFIG[path_key].mkdir(parents=True, exist_ok=True)"],"metadata":{"id":"nGMxMCUCy1R0"},"id":"nGMxMCUCy1R0","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_keyframes(video_path, save_dir, n=4):\n","    save_dir = Path(save_dir)\n","    save_dir.mkdir(parents=True, exist_ok=True)\n","\n","    cap = cv2.VideoCapture(str(video_path))\n","    if not cap.isOpened():\n","        return None\n","\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    if total_frames < n:\n","        cap.release()\n","        return None\n","\n","    indices = np.linspace(0, total_frames - 1, n).astype(int)\n","    saved_paths = []\n","\n","    for i, idx in enumerate(indices):\n","        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n","        ret, frame = cap.read()\n","        if not ret:\n","            continue\n","\n","        frame_path = save_dir / f\"frame_{i}.jpg\"\n","        cv2.imwrite(str(frame_path), frame)\n","        saved_paths.append(frame_path)\n","\n","    cap.release()\n","    return saved_paths"],"metadata":{"id":"48qmIOGny4gY"},"id":"48qmIOGny4gY","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def describe_single_frame(image_path, processor, model, max_new_tokens=200):\n","    image = Image.open(image_path).convert(\"RGB\")\n","\n","    prompt = (\n","        \"<image>\\n\"\n","        \"Analyze this keyframe and provide descriptions for three specific dimensions.\\n\"\n","        \"Strictly follow this output format:\\n\\n\"\n","        \"Expression: [Describe facial expression. If the face is too small, blurred, or NOT present, strictly output 'Unknown'.]\\n\"\n","        \"Posture: [Describe body actions and gestures. If NO people are present, strictly output 'Unknown'.]\\n\"\n","        \"Environment: [Describe the physical location and list key objects present. Do not describe lighting or colors.]\\n\"\n","    )\n","\n","    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(model.device)\n","\n","    with torch.no_grad():\n","        output_ids = model.generate(\n","            **inputs,\n","            max_new_tokens=max_new_tokens,\n","            do_sample=False,\n","            temperature=0.0,\n","            pad_token_id=processor.tokenizer.eos_token_id,\n","        )\n","\n","    input_token_len = inputs.input_ids.shape[1]\n","    generated_ids = output_ids[:, input_token_len:][0]\n","    raw_output = processor.decode(generated_ids, skip_special_tokens=True).strip()\n","\n","    pattern = r\"Expression:\\s*(.*?)\\s*Posture:\\s*(.*?)\\s*Environment:\\s*(.*)\"\n","    match = re.search(pattern, raw_output, re.DOTALL | re.IGNORECASE)\n","\n","    if match:\n","        return {\n","            \"expression\": match.group(1).strip(),\n","            \"posture\": match.group(2).strip(),\n","            \"environment\": match.group(3).strip()\n","        }\n","    else:\n","        return {\n","            \"expression\": \"Unknown\",\n","            \"posture\": \"Unknown\",\n","            \"environment\": raw_output\n","        }\n","\n","def process_video_frames(video_id, processor, model):\n","    video_dir = CONFIG[\"FRAME_OUTPUT_ROOT\"] / video_id\n","    if not video_dir.exists():\n","        return []\n","\n","    frame_paths = sorted([\n","        p for p in video_dir.iterdir()\n","        if p.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]\n","    ])\n","\n","    descriptions = []\n","    for p in frame_paths:\n","        desc_dict = describe_single_frame(p, processor, model, CONFIG[\"MAX_NEW_TOKENS\"])\n","        item = {\n","            \"frame\": p.name,\n","            \"expression\": desc_dict[\"expression\"],\n","            \"posture\": desc_dict[\"posture\"],\n","            \"environment\": desc_dict[\"environment\"]\n","        }\n","        descriptions.append(item)\n","\n","    return descriptions"],"metadata":{"id":"fkI2XWn9y78t"},"id":"fkI2XWn9y78t","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_visual_embedding(video_id, model_emb):\n","    json_path = CONFIG[\"DESC_OUTPUT_ROOT\"] / f\"{video_id}.json\"\n","    if not json_path.exists():\n","        return None\n","\n","    try:\n","        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n","            frames_data = json.load(f)\n","    except:\n","        return None\n","\n","    descriptions = []\n","    for item in frames_data:\n","        text = f\"{item.get('expression', '')}. {item.get('posture', '')}. {item.get('environment', '')}.\"\n","        descriptions.append(text)\n","\n","    if len(descriptions) < CONFIG[\"NUM_FRAMES\"]:\n","        descriptions += [\"\"] * (CONFIG[\"NUM_FRAMES\"] - len(descriptions))\n","    elif len(descriptions) > CONFIG[\"NUM_FRAMES\"]:\n","        descriptions = descriptions[:CONFIG[\"NUM_FRAMES\"]]\n","\n","    formatted_inputs = [f\"passage: {desc.strip()}\" for desc in descriptions]\n","\n","    embeddings = model_emb.encode(\n","        formatted_inputs,\n","        convert_to_numpy=True,\n","        normalize_embeddings=True\n","    )\n","    return embeddings"],"metadata":{"id":"WQ8bP6mRzCBv"},"id":"WQ8bP6mRzCBv","execution_count":null,"outputs":[]},{"cell_type":"code","source":["video_files = sorted([\n","    f for f in os.listdir(CONFIG[\"VIDEO_ROOT\"])\n","    if f.lower().endswith((\".mp4\", \".mov\", \".mkv\", \".avi\"))\n","])\n","\n","for vid in tqdm(video_files):\n","    video_path = CONFIG[\"VIDEO_ROOT\"] / vid\n","    video_id = video_path.stem\n","    frame_dir = CONFIG[\"FRAME_OUTPUT_ROOT\"] / video_id\n","\n","    if frame_dir.is_dir() and len(list(frame_dir.glob(\"*.jpg\"))) >= CONFIG[\"NUM_FRAMES\"]:\n","        continue\n","\n","    extract_keyframes(video_path, frame_dir, n=CONFIG[\"NUM_FRAMES\"])"],"metadata":{"id":"-jFb9_3HzFBt"},"id":"-jFb9_3HzFBt","execution_count":null,"outputs":[]},{"cell_type":"code","source":["bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.float16,\n",")\n","\n","processor = AutoProcessor.from_pretrained(CONFIG[\"LLAVA_MODEL_ID\"])\n","model = AutoModelForVision2Seq.from_pretrained(\n","    CONFIG[\"LLAVA_MODEL_ID\"],\n","    quantization_config=bnb_config,\n","    device_map=\"auto\",\n",")\n","\n","video_ids = sorted([p.name for p in CONFIG[\"FRAME_OUTPUT_ROOT\"].iterdir() if p.is_dir()])\n","\n","for i, vid in enumerate(tqdm(video_ids)):\n","    out_path = CONFIG[\"DESC_OUTPUT_ROOT\"] / f\"{vid}.json\"\n","\n","    if out_path.exists():\n","        continue\n","\n","    try:\n","        descriptions = process_video_frames(vid, processor, model)\n","        if descriptions:\n","            with open(out_path, \"w\", encoding=\"utf-8\") as f:\n","                json.dump(descriptions, f, ensure_ascii=False, indent=2)\n","    except Exception:\n","        continue\n","\n","    if i > 0 and i % CONFIG[\"BATCH_SIZE\"] == 0:\n","        torch.cuda.empty_cache()\n","\n","del model\n","del processor\n","gc.collect()\n","torch.cuda.empty_cache()"],"metadata":{"id":"gsNncBQYzJ71"},"id":"gsNncBQYzJ71","execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_emb = SentenceTransformer(CONFIG[\"EMBEDDING_MODEL_ID\"], device=DEVICE)\n","json_files = sorted(list(CONFIG[\"DESC_OUTPUT_ROOT\"].glob(\"*.json\")))\n","\n","for json_file in tqdm(json_files):\n","    video_id = json_file.stem\n","    save_path = CONFIG[\"FEATURE_OUTPUT_ROOT\"] / f\"{video_id}.npy\"\n","\n","    if save_path.exists():\n","        continue\n","\n","    visual_emb = generate_visual_embedding(video_id, model_emb)\n","\n","    if visual_emb is not None:\n","        np.save(save_path, visual_emb)"],"metadata":{"id":"3jbyIhgNzNQK"},"id":"3jbyIhgNzNQK","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python (vem)","language":"python","name":"vem"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.19"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}