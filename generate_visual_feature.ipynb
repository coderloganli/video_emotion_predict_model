{"cells":[{"cell_type":"code","source":["import os\n","import cv2\n","import torch\n","import numpy as np\n","from tqdm import tqdm\n","from transformers import VideoMAEModel\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","\n","CONFIG = {\n","    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n","    'base_dir': '/content/drive/MyDrive/VEA/',\n","    'video_subdir': 'data16',\n","    'output_subdir': 'features_visual',\n","    'model_name': 'MCG-NJU/videomae-base',\n","    'num_segments': 8,\n","    'frames_per_segment': 16,\n","    'target_size': 224,\n","    'mini_batch_size': 4\n","}\n","\n","def sample_video_frames(video_path):\n","    cap = cv2.VideoCapture(video_path)\n","    if not cap.isOpened():\n","        return None\n","\n","    v_len = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    total_frames = CONFIG['num_segments'] * CONFIG['frames_per_segment']\n","    indices = np.linspace(0, v_len - 1, total_frames).astype(int)\n","\n","    frames = []\n","    for i in indices:\n","        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n","        ret, frame = cap.read()\n","        if not ret:\n","            frame = np.zeros((CONFIG['target_size'], CONFIG['target_size'], 3), dtype=np.uint8)\n","        else:\n","            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            frame = cv2.resize(frame, (CONFIG['target_size'], CONFIG['target_size']), interpolation=cv2.INTER_LINEAR)\n","        frames.append(frame)\n","    cap.release()\n","\n","    frames = np.stack(frames) / 255.0\n","    frames = torch.tensor(frames).float()\n","    frames = frames.permute(0, 3, 1, 2)\n","    frames = frames.reshape(CONFIG['num_segments'], CONFIG['frames_per_segment'], 3, CONFIG['target_size'], CONFIG['target_size'])\n","    return frames\n","\n","def extract_video_feature(model, video_tensor, device):\n","    num_segments = video_tensor.shape[0]\n","    features_list = []\n","\n","    if not video_tensor.is_contiguous():\n","        video_tensor = video_tensor.contiguous()\n","\n","    with torch.no_grad():\n","        for i in range(0, num_segments, CONFIG['mini_batch_size']):\n","            batch_slice = video_tensor[i : i + CONFIG['mini_batch_size']].to(device)\n","            outputs = model(pixel_values=batch_slice)\n","            cls_tokens = outputs.last_hidden_state[:, 0, :]\n","            features_list.append(cls_tokens.cpu().numpy())\n","\n","    return np.concatenate(features_list, axis=0)\n","\n","def main():\n","    device = torch.device(CONFIG['device'])\n","\n","    video_dir = os.path.join(CONFIG['base_dir'], CONFIG['video_subdir'])\n","    output_dir = os.path.join(CONFIG['base_dir'], CONFIG['output_subdir'])\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    model = VideoMAEModel.from_pretrained(CONFIG['model_name']).to(device)\n","    model.eval()\n","\n","    video_files = [f for f in os.listdir(video_dir) if f.endswith(('.mp4', '.avi', '.mov', '.mkv'))]\n","\n","    for vf in tqdm(video_files):\n","        out_path = os.path.join(output_dir, os.path.splitext(vf)[0] + \".npy\")\n","        if os.path.exists(out_path):\n","            continue\n","\n","        try:\n","            video_path = os.path.join(video_dir, vf)\n","            video_tensor = sample_video_frames(video_path)\n","\n","            if video_tensor is None:\n","                continue\n","\n","            feature = extract_video_feature(model, video_tensor, device)\n","            np.save(out_path, feature)\n","\n","        except Exception:\n","            torch.cuda.empty_cache()\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"6waI399TwS58"},"id":"6waI399TwS58","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python (vem)","language":"python","name":"vem"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.19"},"colab":{"provenance":[],"gpuType":"L4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}