{"cells":[{"cell_type":"code","source":["import os\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.metrics import accuracy_score, f1_score\n","from tqdm import tqdm\n","from collections import Counter\n","from google.colab import drive\n","import shutil\n","import time\n","import subprocess\n","\n","def set_seed(seed=42):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    np.random.seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","set_seed(42)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","drive.mount('/content/drive')\n","\n","BASE_PROJECT_DIR = '/content/drive/MyDrive/VEA'\n","LOCAL_ROOT = '/content/temp_data'\n","\n","PATHS = {\n","    \"semantic_dir\": os.path.join(BASE_PROJECT_DIR, \"features_semantic\"),\n","    \"visual_dir\": os.path.join(BASE_PROJECT_DIR, \"features_visual\"),\n","    \"audio_dir\": os.path.join(BASE_PROJECT_DIR, \"features_audio\"),\n","    \"train_labels\": os.path.join(BASE_PROJECT_DIR, \"labels/15_train_labels.csv\"),\n","    \"test_labels\": os.path.join(BASE_PROJECT_DIR, \"labels/15_test_labels.csv\"),\n","    \"model_save_dir\": os.path.join(BASE_PROJECT_DIR, \"models\"),\n","    \"analysis_save_dir\": os.path.join(BASE_PROJECT_DIR, \"analysis\"),\n","}\n","\n","os.makedirs(PATHS[\"model_save_dir\"], exist_ok=True)\n","os.makedirs(PATHS[\"analysis_save_dir\"], exist_ok=True)\n","\n","def accelerate_io(paths_config):\n","    print(\"Initiating data transfer to local runtime for I/O acceleration...\")\n","    start_total = time.time()\n","    feature_keys = [\"semantic_dir\", \"visual_dir\", \"audio_dir\"]\n","\n","    for key in feature_keys:\n","        drive_path = paths_config[key]\n","        folder_name = os.path.basename(drive_path)\n","        local_path = os.path.join(LOCAL_ROOT, folder_name)\n","\n","        if not os.path.exists(local_path):\n","            print(f\" -> Copying {folder_name}...\")\n","            os.makedirs(local_path, exist_ok=True)\n","            try:\n","                subprocess.run([\"cp\", \"-r\", drive_path, os.path.dirname(local_path)], check=True)\n","            except subprocess.CalledProcessError:\n","                print(f\" [WARNING] Failed to copy {folder_name} using cp. Trying shutil.copytree.\")\n","                try:\n","                    shutil.copytree(drive_path, local_path)\n","                except FileNotFoundError:\n","                    print(f\" [WARNING] Drive path not found: {drive_path}. Skipping.\")\n","                    continue\n","        else:\n","            print(f\" -> {folder_name} already exists locally. Skipping copy.\")\n","\n","        paths_config[key] = local_path\n","\n","    local_labels_dir = os.path.join(LOCAL_ROOT, \"labels\")\n","    os.makedirs(local_labels_dir, exist_ok=True)\n","\n","    label_keys = [\"train_labels\", \"test_labels\"]\n","    for key in label_keys:\n","        drive_path = paths_config[key]\n","        file_name = os.path.basename(drive_path)\n","        local_path = os.path.join(local_labels_dir, file_name)\n","\n","        if not os.path.exists(local_path):\n","            print(f\" -> Copying {file_name}...\")\n","            shutil.copy2(drive_path, local_path)\n","        else:\n","            print(f\" -> {file_name} already exists locally. Skipping copy.\")\n","\n","        paths_config[key] = local_path\n","\n","\n","    print(f\"\\nData preparation completed. Time elapsed: {time.time() - start_total:.2f} seconds\")\n","    return paths_config\n","\n","PATHS = accelerate_io(PATHS)\n","\n","df_train = pd.read_csv(PATHS[\"train_labels\"])\n","\n","def compute_class_weights(df, target_col):\n","    labels = df[target_col].values\n","    class_counts = Counter(labels)\n","    total_samples = len(labels)\n","    num_classes = 3\n","    weights = []\n","\n","    for cls_val in sorted(class_counts.keys()):\n","        count = class_counts.get(cls_val, 0)\n","        w = total_samples / (num_classes * count)\n","        weights.append(w)\n","\n","    return torch.tensor(weights, dtype=torch.float)\n","\n","try:\n","    # Use the new column names for weight calculation\n","    valence_weights = compute_class_weights(df_train, 'valenceClass')\n","    arousal_weights = compute_class_weights(df_train, 'arousalClass')\n","except KeyError:\n","    print(\"[WARNING] Could not compute class weights using 'valenceClass' or 'arousalClass'. Using uniform weights.\")\n","    valence_weights = torch.tensor([1.0, 1.0, 1.0], dtype=torch.float)\n","    arousal_weights = torch.tensor([1.0, 1.0, 1.0], dtype=torch.float)\n","\n","CONFIG = {\n","    \"visual_dim\": 768,\n","    \"audio_dim\": 768,\n","    \"semantic_dim\": 1024,\n","    \"lstm_hidden_dim\": 128,\n","    \"lstm_layers\": 1,\n","    \"dropout\": 0.5,\n","    \"batch_size\": 32,\n","    \"learning_rate\": 1e-4,\n","    \"epochs\": 50,\n","    \"patience\": 10,\n","    \"weight_decay\": 1e-3,\n","    \"num_classes\": 3,\n","    \"valence_weights\": valence_weights,\n","    \"arousal_weights\": arousal_weights,\n","}\n","\n","class MultimodalDataset(Dataset):\n","    def __init__(self, labels_path, paths_config):\n","        self.df = pd.read_csv(labels_path)\n","        self.video_ids = self.df['video_id'].astype(str).str.strip().values\n","\n","        # USE NEW COLUMN NAMES: 'valenceClass' and 'arousalClass'\n","        self.valence_labels = torch.tensor(self.df['valenceClass'].values + 1, dtype=torch.long)\n","        self.arousal_labels = torch.tensor(self.df['arousalClass'].values + 1, dtype=torch.long)\n","\n","        self.visual_feats = []\n","        self.audio_feats = []\n","        self.semantic_feats = []\n","\n","        print(f\"Loading data from: {os.path.basename(labels_path)}...\")\n","\n","        loaded_vids = []\n","        for i, vid in enumerate(tqdm(self.video_ids, desc=\"Loading Features to RAM\")):\n","            try:\n","                v_path = os.path.join(paths_config[\"visual_dir\"], f\"{vid}.npy\")\n","                a_path = os.path.join(paths_config[\"audio_dir\"], f\"{vid}.npy\")\n","                s_path = os.path.join(paths_config[\"semantic_dir\"], f\"{vid}.npy\")\n","\n","                self.visual_feats.append(torch.from_numpy(np.load(v_path)).float())\n","                self.audio_feats.append(torch.from_numpy(np.load(a_path)).float())\n","                self.semantic_feats.append(torch.from_numpy(np.load(s_path)).float())\n","                loaded_vids.append(i)\n","            except FileNotFoundError as e:\n","                print(f\" [ERROR] File missing for video {vid}: {e}. Skipping this sample.\")\n","\n","        self.valence_labels = self.valence_labels[loaded_vids]\n","        self.arousal_labels = self.arousal_labels[loaded_vids]\n","        self.video_ids = self.video_ids[loaded_vids]\n","\n","\n","    def __len__(self):\n","        return len(self.visual_feats)\n","\n","    def __getitem__(self, idx):\n","        return {\n","            'visual': self.visual_feats[idx],\n","            'audio': self.audio_feats[idx],\n","            'semantic': self.semantic_feats[idx],\n","            'valence': self.valence_labels[idx],\n","            'arousal': self.arousal_labels[idx]\n","        }\n","\n","print(\"\\nInitializing Train Loader...\")\n","train_dataset = MultimodalDataset(PATHS[\"train_labels\"], PATHS)\n","train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True, num_workers=2)\n","\n","print(\"\\nInitializing Test Loader...\")\n","test_dataset = MultimodalDataset(PATHS[\"test_labels\"], PATHS)\n","test_loader = DataLoader(test_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=2)\n","\n","print(\"\\nDataLoaders successfully initialized.\")"],"metadata":{"id":"0YKcTcttzvRe"},"id":"0YKcTcttzvRe","execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LateFusionBaseline(nn.Module):\n","    def __init__(self, config):\n","        super(LateFusionBaseline, self).__init__()\n","\n","        self.lstm_v = nn.LSTM(\n","            input_size=config[\"visual_dim\"],\n","            hidden_size=config[\"lstm_hidden_dim\"],\n","            num_layers=config[\"lstm_layers\"],\n","            batch_first=True,\n","            bidirectional=True\n","        )\n","\n","        self.lstm_a = nn.LSTM(\n","            input_size=config[\"audio_dim\"],\n","            hidden_size=config[\"lstm_hidden_dim\"],\n","            num_layers=config[\"lstm_layers\"],\n","            batch_first=True,\n","            bidirectional=True\n","        )\n","\n","        self.lstm_s = nn.LSTM(\n","            input_size=config[\"semantic_dim\"],\n","            hidden_size=config[\"lstm_hidden_dim\"],\n","            num_layers=config[\"lstm_layers\"],\n","            batch_first=True,\n","            bidirectional=True\n","        )\n","\n","        fusion_dim = config[\"lstm_hidden_dim\"] * 2 * 3\n","        self.dropout = nn.Dropout(config[\"dropout\"])\n","\n","        self.classifier_valence = nn.Sequential(\n","            nn.Linear(fusion_dim, 128),\n","            nn.ReLU(),\n","            nn.Dropout(config[\"dropout\"]),\n","            nn.Linear(128, config[\"num_classes\"])\n","        )\n","\n","        self.classifier_arousal = nn.Sequential(\n","            nn.Linear(fusion_dim, 128),\n","            nn.ReLU(),\n","            nn.Dropout(config[\"dropout\"]),\n","            nn.Linear(128, config[\"num_classes\"])\n","        )\n","\n","    def forward(self, x_v, x_a, x_s):\n","        self.lstm_v.flatten_parameters()\n","        _, (h_v, _) = self.lstm_v(x_v)\n","        feat_v = torch.cat((h_v[-2,:,:], h_v[-1,:,:]), dim=1)\n","\n","        self.lstm_a.flatten_parameters()\n","        _, (h_a, _) = self.lstm_a(x_a)\n","        feat_a = torch.cat((h_a[-2,:,:], h_a[-1,:,:]), dim=1)\n","\n","        self.lstm_s.flatten_parameters()\n","        _, (h_s, _) = self.lstm_s(x_s)\n","        feat_s = torch.cat((h_s[-2,:,:], h_s[-1,:,:]), dim=1)\n","\n","        fusion_vec = torch.cat((feat_v, feat_a, feat_s), dim=1)\n","        fusion_vec = self.dropout(fusion_vec)\n","\n","        out_valence = self.classifier_valence(fusion_vec)\n","        out_arousal = self.classifier_arousal(fusion_vec)\n","\n","        return out_valence, out_arousal\n","\n","class FocalLoss(nn.Module):\n","    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n","        super(FocalLoss, self).__init__()\n","        self.alpha = alpha\n","        self.gamma = gamma\n","        self.reduction = reduction\n","\n","    def forward(self, logits, targets):\n","        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n","        pt = torch.exp(-ce_loss)\n","        focal_term = (1 - pt) ** self.gamma\n","\n","        if self.alpha is not None:\n","            if self.alpha.device != logits.device:\n","                self.alpha = self.alpha.to(logits.device)\n","            alpha_t = self.alpha[targets]\n","            loss = alpha_t * focal_term * ce_loss\n","        else:\n","            loss = focal_term * ce_loss\n","\n","        if self.reduction == 'mean':\n","            return loss.mean()\n","        elif self.reduction == 'sum':\n","            return loss.sum()\n","        else:\n","            return loss\n","\n","def calculate_metrics(y_true, y_pred):\n","    acc = accuracy_score(y_true, y_pred)\n","    f1 = f1_score(y_true, y_pred, average='weighted')\n","    return acc, f1\n","\n","def train_epoch(model, loader, crit_v, crit_a, optimizer, device):\n","    model.train()\n","    running_loss = 0.0\n","    for batch in loader:\n","        x_v = batch['visual'].to(device)\n","        x_a = batch['audio'].to(device)\n","        x_s = batch['semantic'].to(device)\n","        y_v = batch['valence'].to(device)\n","        y_a = batch['arousal'].to(device)\n","\n","        optimizer.zero_grad()\n","        logits_v, logits_a = model(x_v, x_a, x_s)\n","\n","        loss_v = crit_v(logits_v, y_v)\n","        loss_a = crit_a(logits_a, y_a)\n","        loss = loss_v + loss_a\n","\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","    return running_loss / len(loader)\n","\n","def evaluate_epoch(model, loader, crit_v, crit_a, device):\n","    model.eval()\n","    running_loss = 0.0\n","    all_preds_v, all_labels_v = [], []\n","    all_preds_a, all_labels_a = [], []\n","\n","    with torch.no_grad():\n","        for batch in loader:\n","            x_v = batch['visual'].to(device)\n","            x_a = batch['audio'].to(device)\n","            x_s = batch['semantic'].to(device)\n","            y_v = batch['valence'].to(device)\n","            y_a = batch['arousal'].to(device)\n","\n","            logits_v, logits_a = model(x_v, x_a, x_s)\n","\n","            loss_v = crit_v(logits_v, y_v)\n","            loss_a = crit_a(logits_a, y_a)\n","            running_loss += (loss_v + loss_a).item()\n","\n","            all_preds_v.extend(torch.argmax(logits_v, dim=1).cpu().numpy())\n","            all_labels_v.extend(y_v.cpu().numpy())\n","            all_preds_a.extend(torch.argmax(logits_a, dim=1).cpu().numpy())\n","            all_labels_a.extend(y_a.cpu().numpy())\n","\n","    acc_v, f1_v = calculate_metrics(all_labels_v, all_preds_v)\n","    acc_a, f1_a = calculate_metrics(all_labels_a, all_preds_a)\n","    return running_loss / len(loader), acc_v, f1_v, acc_a, f1_a\n","\n","model = LateFusionBaseline(CONFIG).to(device)\n","criterion_v = FocalLoss(alpha=CONFIG[\"valence_weights\"], gamma=2.0)\n","criterion_a = FocalLoss(alpha=CONFIG[\"arousal_weights\"], gamma=2.0)\n","optimizer = optim.Adam(model.parameters(), lr=CONFIG[\"learning_rate\"], weight_decay=CONFIG[\"weight_decay\"])\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n","\n","best_f1 = -1.0\n","patience_counter = 0\n","save_path = os.path.join(PATHS[\"model_save_dir\"], \"late_fusion_lstm_focal.pth\")\n","\n","print(\"-\" * 95)\n","print(f\"{'Epoch':<6} | {'Train Loss':<10} | {'Val Loss':<10} | {'V-Acc':<6} {'V-F1':<6} | {'A-Acc':<6} {'A-F1':<6} | {'LR'}\")\n","print(\"-\" * 95)\n","\n","for epoch in range(CONFIG[\"epochs\"]):\n","    train_loss = train_epoch(model, train_loader, criterion_v, criterion_a, optimizer, device)\n","    val_loss, v_acc_v, v_f1_v, v_acc_a, v_f1_a = evaluate_epoch(model, test_loader, criterion_v, criterion_a, device)\n","\n","    avg_f1 = (v_f1_v + v_f1_a) / 2\n","    scheduler.step(avg_f1)\n","    current_lr = optimizer.param_groups[0]['lr']\n","\n","    print(f\"{epoch+1:<6} | {train_loss:.4f}     | {val_loss:.4f}     | {v_acc_v:.2f}   {v_f1_v:.2f}   | {v_acc_a:.2f}   {v_f1_a:.2f}   | {current_lr:.1e}\")\n","\n","    if avg_f1 > best_f1:\n","        best_f1 = avg_f1\n","        patience_counter = 0\n","        torch.save(model.state_dict(), save_path)\n","    else:\n","        patience_counter += 1\n","\n","    if patience_counter >= CONFIG[\"patience\"]:\n","        print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n","        break\n","\n","print(\"-\" * 95)\n","print(f\"Training finished. Best Average F1: {best_f1:.4f}\")"],"metadata":{"id":"4-ETOq_TzwJy"},"id":"4-ETOq_TzwJy","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import classification_report\n","model_name = \"late_fusion_lstm_focal.pth\"\n","load_path = os.path.join(PATHS[\"model_save_dir\"], model_name)\n","print(f\"Loading best model from {load_path}...\")\n","\n","model.load_state_dict(torch.load(load_path))\n","model.eval()\n","\n","all_preds_v, all_labels_v = [], []\n","all_preds_a, all_labels_a = [], []\n","\n","with torch.no_grad():\n","    for batch in tqdm(test_loader, desc=\"Running Inference\"):\n","        x_v = batch['visual'].to(device)\n","        x_a = batch['audio'].to(device)\n","        x_s = batch['semantic'].to(device)\n","        y_v = batch['valence'].to(device)\n","        y_a = batch['arousal'].to(device)\n","\n","        logits_v, logits_a = model(x_v, x_a, x_s)\n","\n","        preds_v = torch.argmax(logits_v, dim=1).cpu().numpy()\n","        preds_a = torch.argmax(logits_a, dim=1).cpu().numpy()\n","\n","        all_preds_v.extend(preds_v)\n","        all_labels_v.extend(y_v.cpu().numpy())\n","        all_preds_a.extend(preds_a)\n","        all_labels_a.extend(y_a.cpu().numpy())\n","\n","decoded_preds_v = [p - 1 for p in all_preds_v]\n","decoded_true_v  = [l - 1 for l in all_labels_v]\n","decoded_preds_a = [p - 1 for p in all_preds_a]\n","decoded_true_a  = [l - 1 for l in all_labels_a]\n","\n","video_ids = test_loader.dataset.video_ids\n","\n","df_results = pd.DataFrame({\n","    \"video_id\": video_ids,\n","    \"valence_true\": decoded_true_v,\n","    \"valence_pred\": decoded_preds_v,\n","    \"arousal_true\": decoded_true_a,\n","    \"arousal_pred\": decoded_preds_a\n","})\n","\n","df_results[\"valence_correct\"] = df_results[\"valence_true\"] == df_results[\"valence_pred\"]\n","df_results[\"arousal_correct\"] = df_results[\"arousal_true\"] == df_results[\"arousal_pred\"]\n","\n","output_file = os.path.join(PATHS[\"analysis_save_dir\"], \"late_fusion_lstm_test_predictions.csv\")\n","df_results.to_csv(output_file, index=False)\n","print(f\"\\nPredictions saved to: {output_file}\")\n","\n","print(\"\\n\" + \"=\"*30)\n","print(\"FINAL EVALUATION REPORT\")\n","print(\"=\"*30)\n","\n","target_names = ['Negative (-1)', 'Neutral (0)', 'Positive (1)']\n","\n","print(\"\\n--- VALENCE ---\")\n","acc_v = accuracy_score(all_labels_v, all_preds_v)\n","print(f\"Accuracy: {acc_v:.4f}\")\n","print(classification_report(all_labels_v, all_preds_v, target_names=target_names))\n","\n","print(\"\\n--- AROUSAL ---\")\n","acc_a = accuracy_score(all_labels_a, all_preds_a)\n","print(f\"Accuracy: {acc_a:.4f}\")\n","print(classification_report(all_labels_a, all_preds_a, target_names=target_names))"],"metadata":{"id":"hoTfx24hz3AH"},"id":"hoTfx24hz3AH","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def run_unimodal_experiment(active_modality, config, train_loader, test_loader, device):\n","    print(f\"\\n\" + \"=\"*40)\n","    print(f\"STARTING EXPERIMENT: {active_modality.upper()} ONLY\")\n","    print(\"=\"*40)\n","\n","    model_ablation = LateFusionBaseline(config).to(device)\n","    crit_v = FocalLoss(alpha=config[\"valence_weights\"], gamma=2.0)\n","    crit_a = FocalLoss(alpha=config[\"arousal_weights\"], gamma=2.0)\n","    optimizer = optim.Adam(model_ablation.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n","\n","    epochs = 20\n","\n","    for epoch in range(epochs):\n","        model_ablation.train()\n","        for batch in train_loader:\n","            x_v = batch['visual'].to(device)\n","            x_a = batch['audio'].to(device)\n","            x_s = batch['semantic'].to(device)\n","            y_v = batch['valence'].to(device)\n","            y_a = batch['arousal'].to(device)\n","\n","            if active_modality != 'visual':\n","                x_v = torch.zeros_like(x_v)\n","            if active_modality != 'audio':\n","                x_a = torch.zeros_like(x_a)\n","            if active_modality != 'semantic':\n","                x_s = torch.zeros_like(x_s)\n","\n","            optimizer.zero_grad()\n","            logits_v, logits_a = model_ablation(x_v, x_a, x_s)\n","\n","            loss_v = crit_v(logits_v, y_v)\n","            loss_a = crit_a(logits_a, y_a)\n","            loss = loss_v + loss_a\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","    model_ablation.eval()\n","    all_preds_v, all_labels_v = [], []\n","    all_preds_a, all_labels_a = [], []\n","\n","    with torch.no_grad():\n","        for batch in test_loader:\n","            x_v = batch['visual'].to(device)\n","            x_a = batch['audio'].to(device)\n","            x_s = batch['semantic'].to(device)\n","            y_v = batch['valence'].to(device)\n","            y_a = batch['arousal'].to(device)\n","\n","            if active_modality != 'visual':\n","                x_v = torch.zeros_like(x_v)\n","            if active_modality != 'audio':\n","                x_a = torch.zeros_like(x_a)\n","            if active_modality != 'semantic':\n","                x_s = torch.zeros_like(x_s)\n","\n","            logits_v, logits_a = model_ablation(x_v, x_a, x_s)\n","\n","            all_preds_v.extend(torch.argmax(logits_v, dim=1).cpu().numpy())\n","            all_labels_v.extend(y_v.cpu().numpy())\n","            all_preds_a.extend(torch.argmax(logits_a, dim=1).cpu().numpy())\n","            all_labels_a.extend(y_a.cpu().numpy())\n","\n","    acc_v, f1_v = calculate_metrics(all_labels_v, all_preds_v)\n","    acc_a, f1_a = calculate_metrics(all_labels_a, all_preds_a)\n","    return acc_v, f1_v, acc_a, f1_a\n","\n","results = {}\n","modalities = ['visual', 'audio', 'semantic']\n","\n","for mod in modalities:\n","    results[mod] = run_unimodal_experiment(mod, CONFIG, train_loader, test_loader, device)\n","\n","print(\"\\n\" + \"=\"*65)\n","print(\"MODALITY CONTRIBUTION ANALYSIS (Unimodal Performance)\")\n","print(\"=\"*65)\n","print(f\"{'Modality':<15} | {'V-Acc':<8} {'V-F1':<8} | {'A-Acc':<8} {'A-F1':<8} | {'Avg F1'}\")\n","print(\"-\" * 65)\n","\n","for mod in modalities:\n","    acc_v, f1_v, acc_a, f1_a = results[mod]\n","    avg_f1 = (f1_v + f1_a) / 2\n","    print(f\"{mod.capitalize():<15} | {acc_v:.4f}   {f1_v:.4f}   | {acc_a:.4f}   {f1_a:.4f}   | {avg_f1:.4f}\")\n","print(\"-\" * 65)"],"metadata":{"id":"fNnqSvDpz6Ti"},"id":"fNnqSvDpz6Ti","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"provenance":[],"gpuType":"L4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}