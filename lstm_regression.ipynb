{"cells":[{"cell_type":"code","source":["import os\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","from tqdm import tqdm\n","import copy\n","import torch.nn.functional as F\n","from google.colab import drive\n","import shutil\n","import time\n","import numpy as np\n","\n","def set_seed(seed=42):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    np.random.seed(seed)\n","\n","set_seed(42)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","if device.type == 'cuda':\n","    print(torch.cuda.get_device_name(0))\n","\n","CONFIG = {\n","    \"visual_dim\": 768,\n","    \"audio_dim\": 768,\n","    \"semantic_dim\": 1024,\n","    \"lstm_hidden_dim\": 128,\n","    \"lstm_layers\": 1,\n","    \"dropout\": 0.5,\n","    \"batch_size\": 64,\n","    \"learning_rate\": 1e-4,\n","    \"epochs\": 50,\n","    \"patience\": 10,\n","    \"weight_decay\": 1e-3,\n","    \"output_dim\": 1\n","}\n","\n","drive.mount('/content/drive')\n","\n","BASE_PROJECT_DIR = '/content/drive/MyDrive/VEA'\n","\n","PATHS = {\n","    \"semantic_dir\": os.path.join(BASE_PROJECT_DIR, \"features_semantic\"),\n","    \"visual_dir\": os.path.join(BASE_PROJECT_DIR, \"features_visual\"),\n","    \"audio_dir\": os.path.join(BASE_PROJECT_DIR, \"features_audio\"),\n","    \"train_labels\": os.path.join(BASE_PROJECT_DIR, \"labels/16_train_labels.csv\"),\n","    \"test_labels\": os.path.join(BASE_PROJECT_DIR, \"labels/16_test_labels.csv\"),\n","    \"model_save_dir\": os.path.join(BASE_PROJECT_DIR, \"models\"),\n","    \"analysis_save_dir\": os.path.join(BASE_PROJECT_DIR, \"analysis\"),\n","}\n","\n","def accelerate_io(paths_config):\n","    print(\"Initiating data transfer to local runtime for I/O acceleration...\")\n","    local_base = '/content/temp_data'\n","    start_total = time.time()\n","    feature_keys = [\"semantic_dir\", \"visual_dir\", \"audio_dir\"]\n","\n","    for key in feature_keys:\n","        drive_path = paths_config[key]\n","        folder_name = os.path.basename(drive_path)\n","        local_path = os.path.join(local_base, folder_name)\n","\n","        if not os.path.exists(local_path):\n","            print(f\" -> Copying {folder_name}...\")\n","            try:\n","                shutil.copytree(drive_path, local_path)\n","            except FileNotFoundError:\n","                print(f\" [WARNING] Drive path not found: {drive_path}. Skipping.\")\n","                continue\n","        else:\n","            print(f\" -> {folder_name} already exists locally. Skipping copy.\")\n","\n","        paths_config[key] = local_path\n","\n","    print(f\"Data preparation completed. Time elapsed: {time.time() - start_total:.2f} seconds\")\n","    return paths_config\n","\n","PATHS = accelerate_io(PATHS)\n","\n","print(\"\\nUpdated feature paths:\")\n","print(f\"Visual: {PATHS['visual_dir']}\")\n","print(f\"Audio:  {PATHS['audio_dir']}\")\n","print(\"-\" * 30)\n","\n","class MultimodalDataset(Dataset):\n","    def __init__(self, labels_path, paths_config):\n","        self.df = pd.read_csv(labels_path)\n","        self.video_ids = self.df['video_id'].astype(str).str.strip().values\n","        self.valence_labels = torch.tensor(self.df['valence'].values, dtype=torch.float)\n","        self.arousal_labels = torch.tensor(self.df['arousal'].values, dtype=torch.float)\n","\n","        self.visual_feats = []\n","        self.audio_feats = []\n","        self.semantic_feats = []\n","\n","        print(f\"Loading data from: {os.path.basename(labels_path)}...\")\n","\n","        for vid in tqdm(self.video_ids, desc=\"Loading Features to RAM\"):\n","            try:\n","                v_path = os.path.join(paths_config[\"visual_dir\"], f\"{vid}.npy\")\n","                a_path = os.path.join(paths_config[\"audio_dir\"], f\"{vid}.npy\")\n","                s_path = os.path.join(paths_config[\"semantic_dir\"], f\"{vid}.npy\")\n","\n","                self.visual_feats.append(torch.from_numpy(np.load(v_path)).float())\n","                self.audio_feats.append(torch.from_numpy(np.load(a_path)).float())\n","                self.semantic_feats.append(torch.from_numpy(np.load(s_path)).float())\n","            except FileNotFoundError as e:\n","                print(f\"File missing: {vid} - {e}\")\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        return {\n","            'visual': self.visual_feats[idx],\n","            'audio': self.audio_feats[idx],\n","            'semantic': self.semantic_feats[idx],\n","            'valence': self.valence_labels[idx],\n","            'arousal': self.arousal_labels[idx]\n","        }\n","\n","CONFIG[\"batch_size\"] = 32\n","\n","print(\"\\nInitializing Train Loader...\")\n","try:\n","    train_dataset = MultimodalDataset(PATHS[\"train_labels\"], PATHS)\n","    train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True, num_workers=2)\n","\n","    print(\"Initializing Test Loader...\")\n","    test_dataset = MultimodalDataset(PATHS[\"test_labels\"], PATHS)\n","    test_loader = DataLoader(test_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=2)\n","\n","    print(\"\\nDataLoaders successfully initialized.\")\n","\n","except Exception as e:\n","    print(f\"Data loading error: {e}\")"],"metadata":{"id":"TRt7fXqk8x79"},"id":"TRt7fXqk8x79","execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LateFusionBaseline(nn.Module):\n","    def __init__(self, config):\n","        super(LateFusionBaseline, self).__init__()\n","\n","        self.lstm_v = nn.LSTM(\n","            input_size=config[\"visual_dim\"],\n","            hidden_size=config[\"lstm_hidden_dim\"],\n","            num_layers=config[\"lstm_layers\"],\n","            batch_first=True,\n","            bidirectional=True\n","        )\n","\n","        self.lstm_a = nn.LSTM(\n","            input_size=config[\"audio_dim\"],\n","            hidden_size=config[\"lstm_hidden_dim\"],\n","            num_layers=config[\"lstm_layers\"],\n","            batch_first=True,\n","            bidirectional=True\n","        )\n","\n","        self.lstm_s = nn.LSTM(\n","            input_size=config[\"semantic_dim\"],\n","            hidden_size=config[\"lstm_hidden_dim\"],\n","            num_layers=config[\"lstm_layers\"],\n","            batch_first=True,\n","            bidirectional=True\n","        )\n","\n","        fusion_dim = config[\"lstm_hidden_dim\"] * 2 * 3\n","        self.dropout = nn.Dropout(config[\"dropout\"])\n","\n","        self.regressor_valence = nn.Sequential(\n","            nn.Linear(fusion_dim, 128),\n","            nn.ReLU(),\n","            nn.Dropout(config[\"dropout\"]),\n","            nn.Linear(128, 1)\n","        )\n","\n","        self.regressor_arousal = nn.Sequential(\n","            nn.Linear(fusion_dim, 128),\n","            nn.ReLU(),\n","            nn.Dropout(config[\"dropout\"]),\n","            nn.Linear(128, 1)\n","        )\n","\n","    def forward(self, x_v, x_a, x_s):\n","        self.lstm_v.flatten_parameters()\n","        _, (h_v, _) = self.lstm_v(x_v)\n","        feat_v = torch.cat((h_v[-2,:,:], h_v[-1,:,:]), dim=1)\n","\n","        self.lstm_a.flatten_parameters()\n","        _, (h_a, _) = self.lstm_a(x_a)\n","        feat_a = torch.cat((h_a[-2,:,:], h_a[-1,:,:]), dim=1)\n","\n","        self.lstm_s.flatten_parameters()\n","        _, (h_s, _) = self.lstm_s(x_s)\n","        feat_s = torch.cat((h_s[-2,:,:], h_s[-1,:,:]), dim=1)\n","\n","        fusion_vec = torch.cat((feat_v, feat_a, feat_s), dim=1)\n","        fusion_vec = self.dropout(fusion_vec)\n","\n","        val_pred = self.regressor_valence(fusion_vec).squeeze(-1)\n","        aro_pred = self.regressor_arousal(fusion_vec).squeeze(-1)\n","\n","        return val_pred, aro_pred\n","\n","def calculate_regression_metrics(y_true, y_pred):\n","    y_true = np.asarray(y_true)\n","    y_pred = np.asarray(y_pred)\n","\n","    mse = mean_squared_error(y_true, y_pred)\n","    mae = mean_absolute_error(y_true, y_pred)\n","    return mse, mae\n","\n","def train_one_epoch(model, loader, criterion, optimizer, device):\n","    model.train()\n","    running_loss = 0.0\n","\n","    for batch in tqdm(loader, desc=\"Training\"):\n","        x_v = batch['visual'].to(device)\n","        x_a = batch['audio'].to(device)\n","        x_s = batch['semantic'].to(device)\n","        y_v = batch['valence'].to(device)\n","        y_a = batch['arousal'].to(device)\n","\n","        optimizer.zero_grad()\n","        pred_v, pred_a = model(x_v, x_a, x_s)\n","\n","        loss_v = criterion(pred_v, y_v)\n","        loss_a = criterion(pred_a, y_a)\n","        loss = loss_v + loss_a\n","\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","    return running_loss / len(loader)\n","\n","def evaluate(model, data_loader, criterion, device):\n","    model.eval()\n","    total_loss = 0.0\n","\n","    all_v_preds = []\n","    all_v_targets = []\n","    all_a_preds = []\n","    all_a_targets = []\n","\n","    with torch.no_grad():\n","        for batch in data_loader:\n","            x_v = batch['visual'].to(device)\n","            x_a = batch['audio'].to(device)\n","            x_s = batch['semantic'].to(device)\n","            v_targets = batch['valence'].to(device)\n","            a_targets = batch['arousal'].to(device)\n","\n","            v_preds, a_preds = model(x_v, x_a, x_s)\n","\n","            loss = criterion(v_preds, v_targets) + criterion(a_preds, a_targets)\n","            total_loss += loss.item() * v_targets.size(0)\n","\n","            all_v_preds.append(v_preds.cpu())\n","            all_v_targets.append(v_targets.cpu())\n","            all_a_preds.append(a_preds.cpu())\n","            all_a_targets.append(a_targets.cpu())\n","\n","    v_preds_all = torch.cat(all_v_preds).squeeze()\n","    v_targets_all = torch.cat(all_v_targets).squeeze()\n","    a_preds_all = torch.cat(all_a_preds).squeeze()\n","    a_targets_all = torch.cat(all_a_targets).squeeze()\n","\n","    val_loss = total_loss / len(data_loader.dataset)\n","\n","    def compute_pcc(preds, targets):\n","        vx = preds - torch.mean(preds)\n","        vy = targets - torch.mean(targets)\n","        cov = torch.sum(vx * vy)\n","        denom = torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2))\n","        return (cov / (denom + 1e-8)).item()\n","\n","    mse_v = nn.MSELoss()(v_preds_all, v_targets_all).item()\n","    mae_v = nn.L1Loss()(v_preds_all, v_targets_all).item()\n","    pcc_v = compute_pcc(v_preds_all, v_targets_all)\n","\n","    mse_a = nn.MSELoss()(a_preds_all, a_targets_all).item()\n","    mae_a = nn.L1Loss()(a_preds_all, a_targets_all).item()\n","    pcc_a = compute_pcc(a_preds_all, a_targets_all)\n","\n","    return val_loss, mse_v, mae_v, pcc_v, mse_a, mae_a, pcc_a\n","\n","class CombinedLoss(nn.Module):\n","    def __init__(self, alpha=1.0, beta=1.0):\n","        super(CombinedLoss, self).__init__()\n","        self.alpha = alpha\n","        self.beta = beta\n","\n","    def forward(self, x, y):\n","        x = x.squeeze()\n","        y = y.squeeze()\n","\n","        mse_loss = torch.mean((x - y) ** 2)\n","\n","        x_mean = torch.mean(x)\n","        y_mean = torch.mean(y)\n","        covariance = torch.mean((x - x_mean) * (y - y_mean))\n","        x_var = torch.mean((x - x_mean) ** 2)\n","        y_var = torch.mean((y - y_mean) ** 2)\n","\n","        numerator = 2 * covariance\n","        denominator = x_var + y_var + (x_mean - y_mean) ** 2 + 1e-8\n","        ccc = numerator / denominator\n","        ccc_loss = 1.0 - ccc\n","\n","        return self.alpha * mse_loss + self.beta * ccc_loss\n","\n","model = LateFusionBaseline(CONFIG).to(device)\n","print(model)\n","\n","criterion = CombinedLoss(alpha=1.0, beta=1.0)\n","optimizer = optim.Adam(model.parameters(), lr=CONFIG[\"learning_rate\"], weight_decay=CONFIG[\"weight_decay\"])\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n","save_path = os.path.join(PATHS[\"model_save_dir\"], \"late_fusion_lstm_combined.pth\")\n","\n","best_val_loss = float('inf')\n","patience_counter = 0\n","\n","print(\"Starting Training with Combined Loss (MSE + CCC)...\")\n","print(\"-\" * 115)\n","print(f\"{'Epoch':<6} | {'Train Loss':<10} | {'Val Loss':<10} | {'V-MSE':<8} {'V-PCC':<8} | {'A-MSE':<8} {'A-PCC':<8} | {'LR'}\")\n","print(\"-\" * 115)\n","\n","for epoch in range(CONFIG[\"epochs\"]):\n","    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n","    val_loss, mse_v, mae_v, pcc_v, mse_a, mae_a, pcc_a = evaluate(model, test_loader, criterion, device)\n","\n","    scheduler.step(val_loss)\n","    current_lr = optimizer.param_groups[0]['lr']\n","\n","    print(f\"{epoch+1:<6} | {train_loss:.4f}     | {val_loss:.4f}     | {mse_v:.4f}   {pcc_v:.4f}   | {mse_a:.4f}   {pcc_a:.4f}   | {current_lr:.1e}\")\n","\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        patience_counter = 0\n","        torch.save(model.state_dict(), save_path)\n","    else:\n","        patience_counter += 1\n","\n","    if patience_counter >= CONFIG[\"patience\"]:\n","        print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n","        break\n","\n","print(\"-\" * 115)\n","print(f\"Training finished. Best Val Combined Loss: {best_val_loss:.4f}\")"],"metadata":{"id":"d16LXqVB83NG"},"id":"d16LXqVB83NG","execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name = \"late_fusion_lstm_combined.pth\"\n","load_path = os.path.join(PATHS[\"model_save_dir\"], model_name)\n","\n","print(f\"Loading best model from {load_path}...\")\n","model.load_state_dict(torch.load(load_path))\n","model.eval()\n","\n","all_preds_v, all_labels_v = [], []\n","all_preds_a, all_labels_a = [], []\n","\n","print(\"Running inference on Test Set...\")\n","with torch.no_grad():\n","    for batch in tqdm(test_loader, desc=\"Inference\"):\n","        x_v = batch['visual'].to(device)\n","        x_a = batch['audio'].to(device)\n","        x_s = batch['semantic'].to(device)\n","\n","        pred_v, pred_a = model(x_v, x_a, x_s)\n","\n","        all_preds_v.extend(pred_v.cpu().numpy())\n","        all_labels_v.extend(batch['valence'].cpu().numpy())\n","        all_preds_a.extend(pred_a.cpu().numpy())\n","        all_labels_a.extend(batch['arousal'].cpu().numpy())\n","\n","video_ids = test_loader.dataset.video_ids\n","\n","df_results = pd.DataFrame({\n","    \"video_id\": video_ids,\n","    \"valence_true\": all_labels_v,\n","    \"valence_pred\": all_preds_v,\n","    \"arousal_true\": all_labels_a,\n","    \"arousal_pred\": all_preds_a\n","})\n","\n","df_results[\"valence_error\"] = abs(df_results[\"valence_true\"] - df_results[\"valence_pred\"])\n","df_results[\"arousal_error\"] = abs(df_results[\"arousal_true\"] - df_results[\"arousal_pred\"])\n","\n","output_file = os.path.join(PATHS[\"analysis_save_dir\"], \"late_fusion_lstm_regression_predictions.csv\")\n","df_results.to_csv(output_file, index=False)\n","print(f\"\\nPredictions saved to: {output_file}\")\n","\n","print(\"\\n\" + \"=\"*30)\n","print(\"FINAL REGRESSION REPORT\")\n","print(\"=\"*30)\n","\n","np_labels_v = np.array(all_labels_v).flatten()\n","np_preds_v = np.array(all_preds_v).flatten()\n","np_labels_a = np.array(all_labels_a).flatten()\n","np_preds_a = np.array(all_preds_a).flatten()\n","\n","mse_v = mean_squared_error(all_labels_v, all_preds_v)\n","mae_v = mean_absolute_error(all_labels_v, all_preds_v)\n","pcc_v = np.corrcoef(np_labels_v, np_preds_v)[0, 1]\n","\n","print(f\"\\n--- VALENCE ---\")\n","print(f\"MSE: {mse_v:.4f}\")\n","print(f\"MAE: {mae_v:.4f}\")\n","print(f\"PCC: {pcc_v:.4f}\")\n","\n","mse_a = mean_squared_error(all_labels_a, all_preds_a)\n","mae_a = mean_absolute_error(all_labels_a, all_preds_a)\n","pcc_a = np.corrcoef(np_labels_a, np_preds_a)[0, 1]\n","\n","print(f\"\\n--- AROUSAL ---\")\n","print(f\"MSE: {mse_a:.4f}\")\n","print(f\"MAE: {mae_a:.4f}\")\n","print(f\"PCC: {pcc_a:.4f}\")"],"metadata":{"id":"8o-_j6LL897K"},"id":"8o-_j6LL897K","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"provenance":[],"gpuType":"L4","generative_ai_disabled":true},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}